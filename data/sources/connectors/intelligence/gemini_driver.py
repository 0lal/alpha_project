import logging
from typing import Dict, Any, Optional, List, Tuple
import json

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø£Ù… Ø§Ù„Ø°ÙŠ ÙŠØ·Ø¨Ù‚ Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø­Ø¯ÙˆØ¯
from connectors.base_connector import BaseConnector

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ Google Gemini
logger = logging.getLogger("Alpha.Drivers.GoogleGemini")

class GeminiDriver(BaseConnector):
    """
    Ø§Ù„Ø°Ø±Ø§Ø¹ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ù† Ø¬ÙˆØ¬Ù„ (Gemini).
    
    Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠØ©:
    1. ØªÙˆÙÙŠØ± ØªØ­Ù„ÙŠÙ„ Ù…Ø§Ù„ÙŠ Ø¹Ù…ÙŠÙ‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Pro Ùˆ Flash.
    2. ØªØ·Ø¨ÙŠÙ‚ "Ù…Ø¶Ø§Ø¯ Ø§Ù„Ø±Ù‚Ø§Ø¨Ø©" Ù„Ø¶Ù…Ø§Ù† Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ù‚Ø§Ø³ÙŠØ©.
    3. Ø§Ù„Ù‡Ø¨ÙˆØ· Ø§Ù„Ø¢Ù…Ù† (Failover): Ø¥Ø°Ø§ ÙƒØ§Ù† Pro Ù…Ø´ØºÙˆÙ„Ø§Ù‹ (2 RPM)ØŒ ÙŠØªÙ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Flash ÙÙˆØ±Ø§Ù‹.
    4. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù‡ÙŠÙƒÙ„ JSON Ø§Ù„Ù…Ø¹Ù‚Ø¯ Ø¬Ø¯Ø§Ù‹ Ø§Ù„Ø®Ø§Øµ Ø¨Ø¬ÙˆØ¬Ù„.
    """

    def __init__(self):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¯Ø±Ø§ÙŠÙØ±.
        """
        # ØªÙ…Ø±ÙŠØ± Ø§Ø³Ù… Ø§Ù„Ù…Ø²ÙˆØ¯ Ù„Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø£Ù… Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† google_gemini_keys.json
        super().__init__("google")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØªØ§Ø­ Ù…Ù† Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø¢Ù…Ù†Ø©
        self.api_key = self.config.get("credentials", {}).get("api_key")
        
        if not self.api_key:
            logger.critical("âŒ FATAL: Google Gemini API Key is missing from secure configuration!")

    def build_url(self, endpoint_key: str) -> str:
        """
        [ØªØ¬Ø§ÙˆØ² Ø¥Ø¬Ø¨Ø§Ø±ÙŠ] Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø·.
        Ø¬ÙˆØ¬Ù„ ØªØªØ·Ù„Ø¨ ÙˆØ¶Ø¹ Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ø§Ø®Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· Ù†ÙØ³Ù‡.
        """
        # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: https://generativelanguage.googleapis.com/v1beta/models
        base_url = "https://generativelanguage.googleapis.com/v1beta/models"
        
        # Ù‡Ù†Ø§ endpoint_key Ø³ÙŠÙƒÙˆÙ† Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù…Ø«Ù„: gemini-1.5-pro Ø£Ùˆ gemini-1.5-flash)
        return f"{base_url}/{endpoint_key}:generateContent"

    def get_default_params(self) -> Dict[str, str]:
        """
        [ØªØ¬Ø§ÙˆØ² Ø¥Ø¬Ø¨Ø§Ø±ÙŠ]
        Ø¬ÙˆØ¬Ù„ ØªØªØ·Ù„Ø¨ ÙˆØ¶Ø¹ Ù…ÙØªØ§Ø­ Ø§Ù„Ù€ API ÙÙŠ Ø§Ù„Ø±Ø§Ø¨Ø· (Query Parameters) ÙˆÙ„ÙŠØ³ ÙÙŠ Ø§Ù„ØªØ±ÙˆÙŠØ³Ø©.
        """
        return {"key": self.api_key}

    def _prepare_request_details(self, endpoint_key: str, params: Dict) -> Tuple[str, str, Dict, Dict]:
        """
        [ØªØ¬Ø§ÙˆØ² Ø£Ù…Ù†ÙŠ - Security Override]
        Ø¯Ù…Ø¬ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù† (Safety Settings) Ù„Ù…Ù†Ø¹ Ø¬ÙˆØ¬Ù„ Ù…Ù† Ø­Ø¸Ø± Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ©.
        """
        url, method, final_params, headers = super()._prepare_request_details(endpoint_key, params)
        
        headers["Content-Type"] = "application/json"
        
        # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù† Ù…Ù† Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† (google_gemini_keys.json)
        safety_config = self.config.get("safety_settings", {})
        
        # 2. Ø¨Ù†Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ Ù„Ø¬ÙˆØ¬Ù„
        safety_settings_payload = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": safety_config.get("HARM_CATEGORY_HARASSMENT", "BLOCK_ONLY_HIGH")},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": safety_config.get("HARM_CATEGORY_HATE_SPEECH", "BLOCK_ONLY_HIGH")},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": safety_config.get("HARM_CATEGORY_SEXUALLY_EXPLICIT", "BLOCK_ONLY_HIGH")},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": safety_config.get("HARM_CATEGORY_DANGEROUS_CONTENT", "BLOCK_ONLY_HIGH")}
        ]

        # 3. Ø­Ù‚Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¨Ø¯Ø§Ø®Ù„ Ø¬Ø³Ù… Ø§Ù„Ø·Ù„Ø¨ (Payload)
        final_params["safetySettings"] = safety_settings_payload

        # Ø¬ÙˆØ¬Ù„ ØªØ³ØªØ®Ø¯Ù… POST Ø¯Ø§Ø¦Ù…Ø§Ù‹
        return url, "POST", final_params, headers

    def fetch(self, endpoint_key: str, **params) -> Optional[str]:
        """
        [ØªØ¬Ø§ÙˆØ² Ø¬Ù†Ø§Ø¦ÙŠ ÙˆÙ‡Ø¨ÙˆØ· Ø¢Ù…Ù† - Auto-Failover Override]
        ÙŠØ±Ø³Ù„ Ø§Ù„Ø·Ù„Ø¨ØŒ ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ù†ØµØŒ ÙˆØ¥Ø°Ø§ Ø§ØµØ·Ø¯Ù… Ø¨Ø­Ø¯ Ø§Ù„Ù€ 2 RPM Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø³Ø®Ø© ProØŒ ÙŠØ­ÙˆÙ„ Ù„Ù€ Flash.
        """
        # 1. Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø¹Ø¨Ø± Ø¬Ø¯Ø§Ø± Ø§Ù„Ø­Ù…Ø§ÙŠØ© ÙÙŠ Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø£Ù…
        result = super().fetch(endpoint_key, **params)
        
        # 2. Ø§Ù„Ù‡Ø¨ÙˆØ· Ø§Ù„Ø¢Ù…Ù† (Failover Strategy)
        # Ø¥Ø°Ø§ Ø¹Ø§Ø¯ NoneØŒ ÙÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø£Ù… Ø§ÙƒØªØ´Ù Ø®Ø·Ø£ (Ù…Ø«Ù„ 429 Too Many Requests)
        if not result and endpoint_key == "gemini-1.5-pro":
            logger.warning("âš ï¸ Gemini 1.5 Pro failed or hit rate limit (2 RPM). Instantly falling back to Gemini 1.5 Flash.")
            
            # ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ø­Ø¯Ø« Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ
            if hasattr(self, 'audit_logger') and self.audit_logger:
                self.audit_logger.log_decision("GEMINI_FAILOVER", "PRO_LIMIT_REACHED", "Switched to FLASH", confidence=1.0)
                
            # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙˆØ±Ø§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Flash (Ø§Ù„Ø°ÙŠ ÙŠÙ…ØªÙ„Ùƒ 15 RPM)
            return super().fetch("gemini-1.5-flash", **params)
            
        # 3. Ø¥Ø°Ø§ ÙØ´Ù„ Flash Ø£ÙŠØ¶Ø§Ù‹ØŒ Ø£Ùˆ ÙØ´Ù„ Ø§Ù„Ø·Ù„Ø¨ Ù„Ø£ÙŠ Ø³Ø¨Ø¨ Ø¢Ø®Ø±ØŒ Ù†ØªÙˆÙ‚Ù Ù‡Ù†Ø§
        if not result or not isinstance(result, dict):
            return None

        # 4. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (Content) Ù…Ù† Ù‡ÙŠÙƒÙ„ Ø¬ÙˆØ¬Ù„ Ø§Ù„Ù…Ø¹Ù‚Ø¯
        try:
            candidates = result.get("candidates", [])
            if not candidates:
                # Ù‡Ø°Ø§ ÙŠØ­Ø¯Ø« ØºØ§Ù„Ø¨Ø§Ù‹ Ø¥Ø°Ø§ ØªÙ… Ø­Ø¸Ø± Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø³Ø¨Ø¨ Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù† Ø±ØºÙ… Ù…Ø­Ø§ÙˆÙ„Ø§ØªÙ†Ø§
                prompt_feedback = result.get("promptFeedback", {})
                logger.error(f"ğŸ›‘ Gemini API Blocked the request. Feedback: {prompt_feedback}")
                return None
                
            parts = candidates[0].get("content", {}).get("parts", [])
            if not parts:
                logger.error("ğŸ›‘ Gemini API Error: Empty parts received.")
                return None
                
            # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„ØµØ§ÙÙŠ
            return parts[0].get("text", "").strip()

        except Exception as e:
            logger.error(f"ğŸ›‘ Gemini JSON Parsing Error: {str(e)}")
            if hasattr(self, 'audit_logger') and self.audit_logger:
                self.audit_logger.log_error("GEMINI_PARSE_ERROR", "Failed to extract AI content", str(e))
            return None

    # =========================================================================
    # Ø£Ø°Ø±Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ (Intelligence Arms)
    # =========================================================================

    def analyze_complex_scenario(self, system_prompt: str, scenario_data: str) -> Optional[str]:
        """
        ØªØ­Ù„ÙŠÙ„ Ù…Ø§Ù„ÙŠ Ø¹Ù…ÙŠÙ‚ ÙˆÙ…Ø¹Ù‚Ø¯ (Deep Reasoning).
        ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ `gemini-1.5-pro` Ø§Ù„Ù‚ÙˆÙŠ Ø¬Ø¯Ø§Ù‹ (ÙˆÙ„ÙƒÙ† Ø§Ù„Ø¨Ø·ÙŠØ¡ ÙˆØ§Ù„Ù…Ù‚ÙŠØ¯).
        """
        # ØªØ¬Ù‡ÙŠØ² Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø®Ø§Øµ Ø¨Ø¬ÙˆØ¬Ù„
        payload = {
            "contents": [
                {
                    "role": "user",
                    "parts": [
                        # Ù†Ø¯Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØµØ§Ø±Ù…Ø© Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù…Ù†Ø¹ Ø§Ù„Ù‡Ù„ÙˆØ³Ø©
                        {"text": f"SYSTEM INSTRUCTION: You are a strict financial forensic AI. Base your answers ONLY on facts. No hallucinations.\n\nRULES:\n{system_prompt}\n\nDATA:\n{scenario_data}"}
                    ]
                }
            ],
            "generationConfig": {
                "temperature": 0.2, # Ø­Ø±Ø§Ø±Ø© Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ø§Ù‹ Ù„Ù„ØµØ±Ø§Ù…Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©
                "topP": 0.8,
                "maxOutputTokens": 4096
            }
        }
        
        logger.info("ğŸ§  Requesting Deep Financial Reasoning via Gemini 1.5 Pro")
        return self.fetch("gemini-1.5-pro", **payload)

    def process_large_document(self, system_prompt: str, document_text: str) -> Optional[str]:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†ØµÙˆØµ Ø¶Ø®Ù…Ø© (Ù…Ø«Ù„ ØªÙ‚Ø±ÙŠØ± Ø£Ø±Ø¨Ø§Ø­ Ù…Ù† 50 ØµÙØ­Ø©).
        ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ `gemini-1.5-flash` Ù„Ø£Ù†Ù‡ ÙŠÙ…ØªÙ„Ùƒ Ù†Ø§ÙØ°Ø© Ø³ÙŠØ§Ù‚ Ø¶Ø®Ù…Ø© ÙˆØ³Ø±ÙŠØ¹ Ø¬Ø¯Ø§Ù‹.
        """
        payload = {
            "contents": [
                {
                    "role": "user",
                    "parts": [
                        {"text": f"INSTRUCTION: {system_prompt}\n\nDOCUMENT:\n{document_text}"}
                    ]
                }
            ],
            "generationConfig": {
                "temperature": 0.1, # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ø±ÙÙŠ ÙˆØ¯Ù‚ÙŠÙ‚
                "maxOutputTokens": 8192
            }
        }
        
        logger.info("âš¡ Requesting High-Speed Document Processing via Gemini 1.5 Flash")
        return self.fetch("gemini-1.5-flash", **payload)