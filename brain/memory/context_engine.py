# -*- coding: utf-8 -*-
"""
ALPHA SOVEREIGN - CONTEXT ASSEMBLY ENGINE (VERSION 5.0)
=================================================================
Component: brain/memory/context_engine.py
Role: Ù…Ù‡Ù†Ø¯Ø³ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ (Strategic Context Architect).
Forensic Features:
  - Immutable Context Hashing (ØªÙˆÙ‚ÙŠØ¹ Ø¬Ù†Ø§Ø¦ÙŠ Ù„ÙƒÙ„ Ø³ÙŠØ§Ù‚).
  - Stale Data Rejector (Ø±ÙØ¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©).
  - Smart Token Budgeting (Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ø°ÙƒÙŠØ© Ù„Ù„Ø±Ù…ÙˆØ²).
  - Numpy-Safe Serialization (ØªØ³Ù„Ø³Ù„ Ø¢Ù…Ù† Ù„Ù„Ø£Ø±Ù‚Ø§Ù…).
=================================================================
"""

import logging
import json
import hashlib
import math
from typing import Dict, List, Any, Optional, Union
from datetime import datetime, timedelta, timezone

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ÙƒØªØ¨Ø§Øª Ø­Ø³Ø§Ø¨ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù‚Ø¯Ø± ØªÙ‚Ø±ÙŠØ¨ÙŠ
try:
    import tiktoken
    TOKENIZER_AVAILABLE = True
except ImportError:
    TOKENIZER_AVAILABLE = False

class AlphaJSONEncoder(json.JSONEncoder):
    """
    Ù…Ø­ÙˆÙ„ Ù…Ø®ØµØµ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© (Numpy, Decimal, Datetime).
    ÙŠÙ…Ù†Ø¹ Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ù†Ø¸Ø§Ù… Ø¹Ù†Ø¯ ÙˆØ¬ÙˆØ¯ Ø£Ø±Ù‚Ø§Ù… Ø¹Ù„Ù…ÙŠØ©.
    """
    def default(self, obj):
        if hasattr(obj, 'item'):  # Numpy types
            return obj.item()
        if hasattr(obj, 'isoformat'):  # Datetime
            return obj.isoformat()
        if isinstance(obj, set):
            return list(obj)
        try:
            import numpy as np
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            if isinstance(obj, (np.integer, np.floating)):
                return float(obj)
        except ImportError:
            pass
        return super().default(obj)

class ContextEngine:
    """
    Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ø¥Ù„Ù‰ "Ù…Ø´Ù‡Ø¯ Ø¹Ù‚Ù„ÙŠ" (Mental Scene) Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.
    ÙŠØ¶Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ±Ù‰ ÙÙ‚Ø· Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©ØŒ ÙˆØ§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©ØŒ ÙˆÙ„Ø§ Ø´ÙŠØ¡ ØºÙŠØ±Ù‡Ø§.
    """

    def __init__(self, model_name: str = "gpt-4", max_token_limit: int = 4000):
        self.logger = logging.getLogger("Alpha.Brain.Context")
        self.model_name = model_name
        self.max_tokens = max_token_limit
        self.last_context_hash = None
        
        # Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ø§Ù„Ø±Ù…ÙˆØ² (Token Budget Allocation)
        # Ù†Ø¶Ù…Ù† Ù…Ø³Ø§Ø­Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø±Ø¬Ø© Ø¯Ø§Ø¦Ù…Ø§Ù‹
        self.budget = {
            "system_instructions": 0.15, # 15% Ù„Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ø¹Ù„ÙŠØ§
            "market_hard_data": 0.40,    # 40% Ù„Ù„Ø£Ø±Ù‚Ø§Ù… (ØºÙŠØ± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø­Ø°Ù)
            "episodic_memory": 0.25,     # 25% Ù„Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù‚Ø±ÙŠØ¨Ø©
            "semantic_history": 0.20     # 20% Ù„Ù„ØªØ§Ø±ÙŠØ® (Ø£ÙˆÙ„ Ù…Ø§ ÙŠØªÙ… Ø­Ø°ÙÙ‡)
        }

    async def build_decision_context(self, 
                                     symbol: str, 
                                     market_data: Dict[str, Any], 
                                     episodic_mgr: Any, 
                                     semantic_mgr: Any) -> Dict[str, Any]:
        """
        Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙƒØ¨Ø³ÙˆÙ„Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†Ø²Ø§Ù‡Ø©.
        
        Returns:
            Dict: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 'prompt' (Ø§Ù„Ù†Øµ) Ùˆ 'context_id' (Ù„Ù„ØªØªØ¨Ø¹ Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ).
        """
        
        # 1. ÙØ­Øµ Ø·Ø²Ø§Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Staleness Check)
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ Ø£Ù‚Ø¯Ù… Ù…Ù† 10 Ø«ÙˆØ§Ù†ÙØŒ Ù†Ø±ÙØ¶ Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©
        if not self._is_data_fresh(market_data):
            self.logger.error(f"â›” CRITICAL: Stale market data for {symbol}. Context generation aborted.")
            return {"error": "STALE_DATA_PROTECTION", "prompt": None}

        try:
            # 2. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª (Async Retrieval)
            # Ù†Ø³ØªØ®Ø¯Ù… gather Ù„Ø·Ù„Ø¨ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù‚ØµÙŠØ±Ø© ÙˆØ§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ù„Ù„Ø³Ø±Ø¹Ø©
            # (Ù…Ø­Ø§ÙƒØ§Ø© Ù‡Ù†Ø§ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ¯Ø¹Ù… Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª waitable objects)
            short_term = episodic_mgr.get_current_context() if episodic_mgr else {}
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù…ØªØ¬Ù‡ (Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹: Ø¹Ø¨Ø± Embeddings Ø­Ù‚ÙŠÙ‚ÙŠØ©)
            vector = self._fast_vectorize(market_data)
            long_term = await semantic_mgr.recall_similar_experience(vector) if semantic_mgr else []

            # 3. ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª (Layered Assembly)
            
            # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø£: Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„ØµÙ„Ø¨Ø© (Immutable)
            layer_market = {
                "asset": symbol,
                "t": datetime.now(timezone.utc).isoformat(),
                "price": market_data.get("price"),
                "indicators": market_data.get("technical", {}),
                "order_book_imbalance": market_data.get("ob_imbalance", 0.0)
            }

            # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø¨: Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© (Internal State)
            layer_episodic = {
                "current_position": short_term.get("position", "FLAT"),
                "recent_pnl": short_term.get("pnl_24h", 0.0),
                "risk_budget_remaining": short_term.get("risk_quota", 1.0)
            }

            # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø¬: Ø§Ù„Ø­ÙƒÙ…Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© (Prunable)
            layer_semantic = [
                {"date": m["date"], "outcome": m["outcome"], "similarity": m["score"]}
                for m in long_term
            ]

            # 4. Ø¯Ù…Ø¬ ÙˆØ¶ØºØ· Ø§Ù„Ø³ÙŠØ§Ù‚ (Optimization & Budgeting)
            final_context = self._optimize_context_structure(
                layer_market, layer_episodic, layer_semantic
            )

            # 5. Ø§Ù„ØªÙˆÙ‚ÙŠØ¹ Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ (Forensic Hashing)
            # Ù†Ù†Ø´Ø¦ Ø¨ØµÙ…Ø© ÙØ±ÙŠØ¯Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚. Ø¥Ø°Ø§ Ø­Ø¯Ø« Ø®Ø·Ø£ØŒ Ù†Ø¨Ø­Ø« Ø¨Ù‡Ø°Ø§ Ø§Ù„Ù‡Ø§Ø´ ÙÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª.
            context_str = json.dumps(final_context, cls=AlphaJSONEncoder)
            context_hash = hashlib.sha256(context_str.encode()).hexdigest()[:16]
            self.last_context_hash = context_hash

            return {
                "context_id": context_hash,
                "prompt": context_str,
                "token_count": self._estimate_tokens(context_str)
            }

        except Exception as e:
            self.logger.critical(f"ðŸ”¥ CONTEXT ENGINE MELTDOWN: {e}", exc_info=True)
            # ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„ Ø§Ù„ÙƒØ§Ø±Ø«ÙŠØŒ Ù†Ø¹ÙŠØ¯ Ø³ÙŠØ§Ù‚Ø§Ù‹ ÙØ§Ø±ØºØ§Ù‹ Ø¢Ù…Ù†Ø§Ù‹
            return {"error": str(e), "prompt": None}

    def _is_data_fresh(self, data: Dict[str, Any], max_delay_sec: int = 15) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ Ù…Ù† ÙˆÙ‚Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        ts = data.get("timestamp") or data.get("time") or data.get("t")
        if not ts:
            return True # ØªØ¬Ø§ÙˆØ² Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø·Ø§Ø¨Ø¹ Ø²Ù…Ù†ÙŠ (Ø®Ø·Ø±ØŒ Ù„ÙƒÙ† Ù…Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±)
        
        try:
            # Ø¯Ø¹Ù… Ù„Ù„Ø£Ø±Ù‚Ø§Ù… (Unix Timestamp) ÙˆØ§Ù„Ù†ØµÙˆØµ (ISO)
            if isinstance(ts, (int, float)):
                # ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ù‚Ù… ÙƒØ¨ÙŠØ±Ø§Ù‹ Ø¬Ø¯Ø§Ù‹
                if ts > 1e11: ts /= 1000 
                data_time = datetime.fromtimestamp(ts, tz=timezone.utc)
            else:
                data_time = datetime.fromisoformat(str(ts).replace('Z', '+00:00'))
            
            # Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ UTC
            delta = datetime.now(timezone.utc) - data_time
            if delta.total_seconds() > max_delay_sec:
                self.logger.warning(f"Data lag detected: {delta.total_seconds():.2f}s (Max: {max_delay_sec}s)")
                return False
            return True
        except Exception:
            # Ø¥Ø°Ø§ ÙØ´Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆÙ‚ØªØŒ Ù†ÙØªØ±Ø¶ Ø§Ù„Ø£Ù…Ø§Ù† Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„ÙƒÙ„ÙŠØŒ Ù„ÙƒÙ† Ù†Ø³Ø¬Ù„ ØªØ­Ø°ÙŠØ±
            self.logger.warning(f"Timestamp parsing failed for: {ts}")
            return True

    def _optimize_context_structure(self, market: Dict, episodic: Dict, semantic: List) -> Dict:
        """
        ØªÙ‚Ù„ÙŠÙ… Ø°ÙƒÙŠ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆÙ„ÙŠØ³ Ø§Ù„Ø­Ø°Ù Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ.
        """
        # Ø­Ø³Ø§Ø¨ Ù…Ø¨Ø¯Ø¦ÙŠ
        base = {"market": market, "internal": episodic}
        base_tokens = self._estimate_tokens(json.dumps(base, cls=AlphaJSONEncoder))
        
        remaining_budget = self.max_tokens - base_tokens
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¨Ù‚Ø¯Ø± Ù…Ø§ ØªØ³Ù…Ø­ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©
        fitted_history = []
        for item in semantic:
            item_tokens = self._estimate_tokens(json.dumps(item, cls=AlphaJSONEncoder))
            if remaining_budget >= item_tokens:
                fitted_history.append(item)
                remaining_budget -= item_tokens
            else:
                break # ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø§Ù…ØªÙ„Ø§Ø¡ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        
        return {
            "role": "SYSTEM_ALPHA_CORE",
            "market_data": market,
            "agent_state": episodic,
            "historical_analogies": fitted_history
        }

    def _estimate_tokens(self, text: str) -> int:
        """Ø­Ø³Ø§Ø¨ Ø¯Ù‚ÙŠÙ‚ Ø£Ùˆ ØªÙ‚Ø¯ÙŠØ±ÙŠ Ù„Ù„Ø±Ù…ÙˆØ²"""
        if TOKENIZER_AVAILABLE:
            try:
                enc = tiktoken.encoding_for_model(self.model_name)
                return len(enc.encode(text))
            except:
                pass # Fallback to approximation
        
        # ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ: Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ~1.3 ØªÙˆÙƒÙ†ØŒ Ø§Ù„Ø±Ù…ÙˆØ² ~1 ØªÙˆÙƒÙ†
        return int(len(text) / 3.5)

    def _fast_vectorize(self, data: Dict) -> List[float]:
        """
        ØªØ­ÙˆÙŠÙ„ Ø³Ø±ÙŠØ¹ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ù„Ù…ØªØ¬Ù‡ (Ù…Ø¤Ù‚Øª Ù„Ø­ÙŠÙ† ØªØ´ØºÙŠÙ„ DB Vector).
        """
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØªØ·Ø¨ÙŠØ¹Ù‡Ø§ (Normalization) ØªÙ‚Ø±ÙŠØ¨ÙŠØ§Ù‹
            price_change = float(data.get("price_change_24h", 0))
            rsi = float(data.get("technical", {}).get("rsi", 50)) / 100.0
            vol = float(data.get("volatility", 0))
            return [price_change, rsi, vol]
        except:
            return [0.0, 0.5, 0.0]

# =================================================================
# Unit Test (Forensic Verification)
# =================================================================
if __name__ == "__main__":
    import asyncio
    
    async def test_engine():
        print("\n[*] Initializing Context Engine v5.0...")
        engine = ContextEngine(max_token_limit=1000)
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù‡Ø§ Ø£Ø±Ù‚Ø§Ù… Ù…Ø¹Ù‚Ø¯Ø© (Numpy like)
        mock_market = {
            "price": 95432.12,
            "timestamp": datetime.now(timezone.utc).timestamp(),
            "technical": {"rsi": 75.4, "macd": 120.5},
            # Ù…Ø­Ø§ÙƒØ§Ø© Ù‚ÙŠÙ…Ø© ØºÙŠØ± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ³Ù„Ø³Ù„ Ø¹Ø§Ø¯Ø©
            "complex_metric": {1, 2, 3} 
        }
        
        print("[*] Building Context...")
        ctx = await engine.build_decision_context("BTCUSDT", mock_market, None, None)
        
        if "error" in ctx:
            print(f"[-] Test Failed: {ctx['error']}")
        else:
            print(f"[+] Context ID: {ctx['context_id']}")
            print(f"[+] Token Est:  {ctx['token_count']}")
            print(f"[+] Payload:    {ctx['prompt'][:100]}...") # Print first 100 chars
            print("[+] Serialization Check Passed.")

    asyncio.run(test_engine())