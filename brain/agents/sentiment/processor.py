# -*- coding: utf-8 -*-
"""
ALPHA SOVEREIGN - HYBRID SENTIMENT ENGINE (LOCAL + CLOUD)
=================================================================
Component: brain/agents/sentiment/processor.py
Role: Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù‡Ø¬ÙŠÙ† (Hybrid Sentiment Processor).
Forensic Features:
  - Dual-Path Analysis: Ù…Ø³Ø§Ø± Ø³Ø±ÙŠØ¹ (Local) ÙˆÙ…Ø³Ø§Ø± Ø¹Ù…ÙŠÙ‚ (Cloud via Gateway).
  - Cross-Verification: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ.
  - JSON Strict Parsing: Ø¥Ø¬Ø¨Ø§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ Ø¨Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù‡ÙŠÙƒÙ„Ø© ÙÙ‚Ø·.
Integration:
  - Linked to: brain.inference.remote_gateway (SecureCloudGateway)
=================================================================
"""

import logging
import asyncio
import json
import re
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor
import warnings

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ© (Ø§Ù„Ø­Ù„ Ø§Ù„Ø¬Ø°Ø±ÙŠ Ù„Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©)
try:
    from brain.inference.remote_gateway import SecureCloudGateway
    GATEWAY_AVAILABLE = True
except ImportError:
    GATEWAY_AVAILABLE = False

# ØªØ¬Ø§Ù‡Ù„ ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
warnings.filterwarnings("ignore")

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© (Transformer)
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

class HybridSentimentProcessor:
    """
    Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù‡Ø¬ÙŠÙ†.
    ÙŠØ¯Ù…Ø¬ Ø¨ÙŠÙ† Ø³Ø±Ø¹Ø© "FinBERT" ÙˆØ¹Ù…Ù‚ "Gemini/DeepSeek" Ø¹Ø¨Ø± Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø¢Ù…Ù†Ø©.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(HybridSentimentProcessor, cls).__new__(cls)
            cls._instance.initialized = False
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø³Ø§Ø± Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø­Ù„ÙŠ
            cls._instance.executor = ThreadPoolExecutor(max_workers=1)
        return cls._instance

    def __init__(self, local_model_name: str = "ProsusAI/finbert"):
        if self.initialized: return
        
        self.logger = logging.getLogger("Alpha.Brain.Sentiment")
        self.local_model_name = local_model_name
        self.device = "cpu"
        
        # 1. Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ© (The Missing Link Solved)
        if GATEWAY_AVAILABLE:
            self.cloud_gateway = SecureCloudGateway()
            self.logger.info("âœ… Cloud Gateway Linked to Sentiment Engine.")
        else:
            self.cloud_gateway = None
            self.logger.warning("âš ï¸ Cloud Gateway NOT found. Running in Local-Only mode.")

        # Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠ
        self.tokenizer = None
        self.model = None
        self.id2label = {}

    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ (Ù…Ø­Ù„ÙŠ + Ø³Ø­Ø§Ø¨ÙŠ)"""
        if self.initialized: return

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
        if TRANSFORMERS_AVAILABLE:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.logger.info(f"ðŸ§  Loading Local Engine [{self.local_model_name}] on {self.device}...")
            loop = asyncio.get_running_loop()
            await loop.run_in_executor(self.executor, self._load_local_model_sync)
        
        self.initialized = True

    def _load_local_model_sync(self):
        """Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØ²Ø§Ù…Ù† Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠ"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.local_model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.local_model_name).to(self.device)
            self.model.eval()
            config = AutoConfig.from_pretrained(self.local_model_name)
            self.id2label = config.id2label
        except Exception as e:
            self.logger.error(f"Failed to load local model: {e}")

    async def analyze(self, text: str, depth: str = "AUTO") -> Dict[str, Any]:
        """
        ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠØ© Ø§Ù„Ù…ÙˆØ­Ø¯Ø©.
        
        Args:
            text: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡.
            depth: Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ('FAST', 'DEEP', 'AUTO').
                   - FAST: ÙŠØ³ØªØ®Ø¯Ù… FinBERT ÙÙ‚Ø· (Ø³Ø±ÙŠØ¹ ÙˆÙ…Ø¬Ø§Ù†ÙŠ).
                   - DEEP: ÙŠØ³ØªØ®Ø¯Ù… Gateway LLM (Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…ÙƒÙ„Ù).
                   - AUTO: ÙŠÙ‚Ø±Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Øµ.
        """
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
        clean_text = text.strip()
        if not clean_text: return {"sentiment": "NEUTRAL", "score": 0.0}

        # Ù‚Ø±Ø§Ø± Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ (Routing Logic)
        use_cloud = False
        if depth == "DEEP":
            use_cloud = True
        elif depth == "AUTO":
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø£Ùˆ Ù…Ø¹Ù‚Ø¯Ø§Ù‹ØŒ Ù†Ø°Ù‡Ø¨ Ù„Ù„Ø³Ø­Ø§Ø¨Ø©
            if len(clean_text.split()) > 20 or "?" in clean_text:
                use_cloud = True

        # 1. Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ (Deep Analysis)
        if use_cloud and self.cloud_gateway:
            try:
                result = await self._analyze_via_cloud(clean_text)
                if result["status"] == "success":
                    return result
            except Exception as e:
                self.logger.warning(f"Cloud analysis failed ({e}), falling back to local.")

        # 2. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠ (Fallback / Fast Path)
        return await self._analyze_via_local(clean_text)

    async def _analyze_via_cloud(self, text: str) -> Dict[str, Any]:
        """
        Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ© (remote_gateway) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³ØªÙˆÙ‰.
        """
        # Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…Ù„Ù‚Ù† (Prompt Engineering) Ù„Ø¥Ø¬Ø¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ Ø¨Ù€ JSON
        prompt = f"""
        Analyze the financial sentiment of this text strictly.
        Text: "{text}"
        
        Rules:
        1. Ignore generic news, focus on market impact.
        2. Return ONLY a JSON object. No markdown, no explanations.
        3. Format: {{"sentiment": "BULLISH" | "BEARISH" | "NEUTRAL", "score": 0.0 to 1.0, "reason": "short explanation"}}
        """

        # Ù†Ø³ØªØ®Ø¯Ù… ÙˆØ¶Ø¹ "reasoning" (deep) Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹ØŒ Ø£Ùˆ "speed"
        response = await self.cloud_gateway.infer(mode="speed", prompt=prompt)
        
        if response.get("status") != "success":
            raise ValueError("Gateway returned error")

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø¯ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ JSON
        raw_content = response["content"]
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ù…Ø· JSON Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ (Ù„Ø£Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ ØªØ¶ÙŠÙ Ù…Ù‚Ø¯Ù…Ø§Øª)
            json_match = re.search(r'\{.*\}', raw_content, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group(0))
                return {
                    "source": "CLOUD_LLM",
                    "sentiment": data.get("sentiment", "NEUTRAL").upper(),
                    "score": float(data.get("score", 0.5)),
                    "reason": data.get("reason", "No reason provided"),
                    "status": "success"
                }
        except json.JSONDecodeError:
            self.logger.error(f"Failed to parse LLM JSON: {raw_content}")
        
        raise ValueError("Invalid LLM response format")

    async def _analyze_via_local(self, text: str) -> Dict[str, Any]:
        """
        Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠ (FinBERT).
        """
        if not self.model or not self.tokenizer:
            return self._fallback_keyword_search(text)

        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(self.executor, self._infer_local_sync, text)

    def _infer_local_sync(self, text: str) -> Dict[str, Any]:
        """ÙƒÙˆØ¯ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠ (Blocking Code wrapped in Executor)"""
        try:
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(self.device)
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()[0]
            top_idx = np.argmax(probs)
            score = float(probs[top_idx])
            label = self.id2label.get(top_idx, str(top_idx)).lower()

            # ØªÙˆØ­ÙŠØ¯ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
            sentiment = "NEUTRAL"
            if "pos" in label: sentiment = "BULLISH"
            elif "neg" in label: sentiment = "BEARISH"

            return {
                "source": "LOCAL_FINBERT",
                "sentiment": sentiment,
                "score": score,
                "reason": "Local Transformer Inference",
                "status": "success"
            }
        except Exception as e:
            self.logger.error(f"Local inference error: {e}")
            return self._fallback_keyword_search(text)

    def _fallback_keyword_search(self, text: str) -> Dict[str, Any]:
        """Ø§Ù„Ù…Ù„Ø§Ø° Ø§Ù„Ø£Ø®ÙŠØ±: Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        text_lower = text.lower()
        if any(w in text_lower for w in ["soar", "surge", "gain", "buy", "record"]):
            return {"source": "KEYWORD", "sentiment": "BULLISH", "score": 0.6, "status": "fallback"}
        if any(w in text_lower for w in ["crash", "drop", "loss", "sell", "fear"]):
            return {"source": "KEYWORD", "sentiment": "BEARISH", "score": 0.6, "status": "fallback"}
        return {"source": "KEYWORD", "sentiment": "NEUTRAL", "score": 0.5, "status": "fallback"}

    async def unload(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        if self.cloud_gateway:
            await self.cloud_gateway.shutdown()
        if self.model:
            del self.model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

# =================================================================
# Forensic Verification (Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø­ÙŠ)
# =================================================================
if __name__ == "__main__":
    # Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ¦Ø© ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    async def test_integration():
        print("\n[*] Initializing Hybrid Sentiment Engine...")
        engine = HybridSentimentProcessor()
        await engine.initialize()
        
        # Ø­Ø§Ù„Ø© 1: Ù†Øµ Ø¨Ø³ÙŠØ· (ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¹Ø§Ù„Ø¬Ù‡ FinBERT Ù…Ø­Ù„ÙŠØ§Ù‹)
        text_simple = "Tesla stock rises 5%."
        print(f"\n[1] Testing FAST mode (Local): '{text_simple}'")
        res_local = await engine.analyze(text_simple, depth="FAST")
        print(f"    -> Result: {res_local['sentiment']} ({res_local['source']})")

        # Ø­Ø§Ù„Ø© 2: Ù†Øµ Ù…Ø¹Ù‚Ø¯ (ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¹Ø§Ù„Ø¬Ù‡ Ø§Ù„Ù€ Gateway)
        # Ù„Ø§Ø­Ø¸: Ù‡Ø°Ø§ Ø³ÙŠÙØ´Ù„ Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙØ§ØªÙŠØ­ API Ù…Ø¶Ø¨ÙˆØ·Ø©ØŒ ÙˆØ³ÙŠØ¹ÙˆØ¯ Ù„Ù„Ù…Ø­Ù„ÙŠ
        text_complex = "Despite the revenue beat, the grim guidance suggests a storm is coming for the tech sector."
        print(f"\n[2] Testing DEEP mode (Cloud Gateway): '{text_complex}'")
        res_cloud = await engine.analyze(text_complex, depth="DEEP")
        print(f"    -> Result: {res_cloud['sentiment']} ({res_cloud['source']})")
        if "reason" in res_cloud:
            print(f"    -> Reason: {res_cloud['reason']}")

        await engine.unload()

    asyncio.run(test_integration())