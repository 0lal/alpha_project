# -*- coding: utf-8 -*-
"""
ALPHA SOVEREIGN - LOCAL INFERENCE ENGINE (THE REACTOR)
======================================================
Path: alpha_project/brain/inference/local_runner.py
Role: ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¶Ø®Ù…Ø© (LLMs) Ù…Ø­Ù„ÙŠØ§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹ØªØ§Ø¯ Ø§Ù„Ù…ØªØ§Ø­ (CPU/GPU).
Dependency: llama-cpp-python

Forensic Features:
  1. **Thread-Safe Inference**: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙÙ„ (Mutex) Ù„Ù…Ù†Ø¹ ØªØ¯Ø§Ø®Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª ÙˆØ¥ØªÙ„Ø§Ù Ø§Ù„Ø°Ø§ÙƒØ±Ø©.
  2. **Memory Guard**: ÙŠÙ…Ù†Ø¹ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªÙƒØ±Ø± Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù„ØªØ¬Ù†Ø¨ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø±Ø§Ù… (OOM Killer).
  3. **Hardware Acceleration**: Ù…Ù‡ÙŠØ£ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU (cuda) ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¥Ø°Ø§ ØªÙˆÙØ±Øª Ø§Ù„ØªØ¹Ø±ÙŠÙØ§Øª.
  4. **Strict Prompting**: ÙŠÙØ±Ø¶ Ù‡ÙŠÙƒÙ„ÙŠØ© DeepSeek Coder Ù„Ø¶Ù…Ø§Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ø§ØªØ¬.

Author: Alpha Architect (AI)
Status: PRODUCTION READY
"""

import os
import logging
import threading
import time
from typing import Dict, Any, Optional

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø®Ø·Ø£ Ø¬Ù†Ø§Ø¦ÙŠØ§Ù‹
try:
    from llama_cpp import Llama
    HAS_LLAMA = True
except ImportError:
    HAS_LLAMA = False

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
from alpha_project.brain.base_agent import BaseAgent
from alpha_project.core.registry import register_component
from alpha_project.ui.core.config_provider import config as sys_config

@register_component(name="brain.local", category="brain", is_critical=False)
class LocalRunner(BaseAgent):
    """
    Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø­Ù„ÙŠ.
    Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† ØªØ´ØºÙŠÙ„ Ù…Ù„ÙØ§Øª GGUF Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ Ø§Ù„ØµÙ„Ø¨.
    """
    
    # [Forensic Evidence]: Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø«Ø§Ø¨Øª Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙƒÙ…Ø§ ÙˆØ¬Ø¯ ÙÙŠ Ø§Ù„ØªØ­Ù‚ÙŠÙ‚Ø§Øª 
    # D:\my_deepseek_model\deepseek-coder-v2-16b-lite-instruct-q5_K_M.gguf
    DEFAULT_MODEL_PATH = r"D:\my_deepseek_model\deepseek-coder-v2-16b-lite-instruct-q5_K_M.gguf"

    def __init__(self):
        super().__init__(name="brain.local", category="brain")
        self._llm: Optional[Llama] = None
        self._lock = threading.Lock()  # Ù‚ÙÙ„ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªØ¶Ø§Ø±Ø¨ Ø¨ÙŠÙ† Ø§Ù„Ø®ÙŠÙˆØ·
        self._model_path = self.DEFAULT_MODEL_PATH

    # =========================================================================
    # 1. Initialization (Ø§Ù„Ø¥Ù‚Ù„Ø§Ø¹ Ø§Ù„Ø¢Ù…Ù†)
    # =========================================================================

    def initialize(self, config: Dict[str, Any]) -> bool:
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©. Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø«Ù‚ÙŠÙ„Ø© ÙˆÙ‚Ø¯ ØªØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹.
        """
        super().initialize(config)
        
        # 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ÙƒØªØ¨Ø©
        if not HAS_LLAMA:
            self._logger.warning("âš ï¸ 'llama-cpp-python' not installed. Local brain disabled.")
            return False

        # 2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù (Digital Forensics)
        # ÙŠÙ…ÙƒÙ† Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… ØªØºÙŠÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø± Ø¹Ø¨Ø± system_manifest.yaml Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹
        custom_path = sys_config.get("brain.models.local.path")
        if custom_path:
            self._model_path = custom_path

        if not os.path.exists(self._model_path):
            self._logger.error(f"âŒ Model file MISSING at: {self._model_path}")
            self._logger.error("   -> Please ensure the .gguf file exists in the drive.")
            return False

        # 3. Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙØ¹Ù„ÙŠ (Heavy Lifting)
        try:
            if self._llm:
                self._logger.info("â™»ï¸ Model already loaded via cache.")
                return True

            self._logger.info(f"â˜¢ï¸  Initializing Local Reactor (Loading GGUF)...")
            self._logger.info(f"    -> Path: {self._model_path}")
            
            start_time = time.time()
            
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ (Performance Tuning)
            # n_gpu_layers=-1 : Ø­Ø§ÙˆÙ„ Ù†Ù‚Ù„ ÙƒÙ„ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ù„Ù„ÙƒØ§Ø±Øª (Ø¥Ø°Ø§ ÙˆØ¬Ø¯)
            # n_ctx=4096      : Ø°Ø§ÙƒØ±Ø© Ø³ÙŠØ§Ù‚ÙŠØ© ÙƒØ§ÙÙŠØ© Ù„Ù„ÙƒÙˆØ¯
            # n_threads=6     : Ø§Ø³ØªØ®Ø¯Ø§Ù… 6 Ø£Ù†ÙˆÙŠØ© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬ (ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„ÙˆÙŠÙ†Ø¯ÙˆØ²)
            
            self._llm = Llama(
                model_path=self._model_path,
                n_ctx=4096, 
                n_threads=6,  
                n_gpu_layers=-1, 
                verbose=False # Ø¥Ø®ÙØ§Ø¡ Ø¶Ø¬ÙŠØ¬ Ø§Ù„Ù…ÙƒØªØ¨Ø© ÙÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
            )
            
            load_time = round(time.time() - start_time, 2)
            self._logger.info(f"âœ… Local Brain ONLINE in {load_time}s.")
            return True

        except Exception as e:
            self._logger.critical(f"ğŸ’¥ REACTOR CORE FAILURE: {e}")
            return False

    def shutdown(self) -> None:
        """ØªØ­Ø±ÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø¥ØºÙ„Ø§Ù‚"""
        if self._llm:
            self._logger.info("ğŸ›‘ Unloading Local Model to free RAM...")
            del self._llm
            self._llm = None
        super().shutdown()

    # =========================================================================
    # 2. Reasoning Execution (Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø­Ù…ÙŠ)
    # =========================================================================

    def _execute_reasoning(self, prompt: str, context: Dict) -> str:
        """
        ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬.
        """
        if not self._llm:
            raise RuntimeError("Local model is not loaded (Initialization failed).")

        # 1. Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙÙ„ (Thread Safety)
        # Ù‡Ø°Ø§ ÙŠÙ…Ù†Ø¹ Ø¨Ø±Ù†Ø§Ù…Ø¬ÙŠÙ† Ù…Ù† Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª
        with self._lock:
            self._logger.debug("ğŸ§  Local Brain is thinking...")
            
            # 2. Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø£ÙˆØ§Ù…Ø± (Prompt Engineering)
            # DeepSeek Coder ÙŠØªØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„ ØªØ­Ø¯ÙŠØ¯Ø§Ù‹ 
            formatted_prompt = self._apply_template(prompt)
            
            try:
                # 3. Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (Generation)
                output = self._llm(
                    formatted_prompt,
                    max_tokens=2000,   # Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø¥Ø¬Ø§Ø¨Ø§Øª Ø·ÙˆÙŠÙ„Ø© Ù„Ù„ÙƒÙˆØ¯
                    temperature=0.2,   # Ø­Ø±Ø§Ø±Ø© Ù…Ù†Ø®ÙØ¶Ø© Ù„Ù„ÙƒÙˆØ¯ (Ù†Ø±ÙŠØ¯ Ø¯Ù‚Ø© Ù„Ø§ Ø¥Ø¨Ø¯Ø§Ø¹)
                    stop=["### Instruction:", "### User:"], # Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚Ù
                    echo=False
                )
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ
                result_text = output['choices'][0]['text'].strip()
                
                # ØªØ³Ø¬ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ù„Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠØ©
                usage = output.get('usage', {})
                self._logger.info(f"âš¡ Generated {usage.get('completion_tokens', 0)} tokens.")
                
                return result_text

            except Exception as e:
                self._logger.error(f"âŒ Generation Error: {e}")
                raise e

    def _apply_template(self, prompt: str) -> str:
        """
        ØªØ·Ø¨ÙŠÙ‚ Ù‚Ø§Ù„Ø¨ DeepSeek-Coder.
        """
        # Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ:
        # ### Instruction:
        # {prompt}
        #
        # ### Response:
        return f"### Instruction:\n{prompt}\n\n### Response:"

# =============================================================================
# Self-Diagnostic (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ù†ÙØµÙ„)
# =============================================================================
if __name__ == "__main__":
    print("ğŸ” Testing LocalRunner independently...")
    runner = LocalRunner()
    
    # Ù…Ø­Ø§ÙƒØ§Ø© Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    if runner.initialize({}):
        print("âœ… Init Success. Testing Generation...")
        response = runner._execute_reasoning("Write a Python function to merge two lists.", {})
        print("\n--- OUTPUT ---\n")
        print(response)
        print("\n--------------")
    else:
        print("âŒ Init Failed.")