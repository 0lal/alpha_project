import os
import json
import pandas as pd
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from PyQt6.QtCore import QObject, pyqtSignal, QMutex, QMutexLocker

# --- Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© ---
from ui.core.config_provider import config
from ui.core.logger_sink import logger_sink
from ui.core.workers import task_manager
from ui.core.stream_handler import stream_handler # Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø£Ø³ÙˆØ¯

class AlphaDataManager(QObject):
    """
    The Custodian of Records.
    
    Ø§Ù„ÙˆØ¸ÙŠÙØ©:
    1. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© (Historical Data) Ù„ØºØ±Ø¶ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ§Ù„Ù…Ø­Ø§ÙƒØ§Ø©.
    2. Ø£Ø±Ø´ÙØ© Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø§Øª (Logs & Blackbox Dumps) Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ.
    3. Ø¶Ù…Ø§Ù† Ù†Ø²Ø§Ù‡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Integrity) ÙˆÙ…Ù†Ø¹ ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª ØªØ§Ù„ÙØ©.
    
    Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ:
    ÙŠÙØ¶Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ†Ø³ÙŠÙ‚ Parquet Ù„Ø£Ù†Ù‡ ÙŠØ­ÙØ¸ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Types) Ø¨Ø¯Ù‚Ø©
    ÙˆÙŠÙ…Ù†Ø¹ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø´Ù‡ÙŠØ±Ø© ÙÙŠ CSV (Ù…Ø«Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø¥Ù„Ù‰ Ù†ØµÙˆØµ).
    """

    # Ø¥Ø´Ø§Ø±Ø© Ø¹Ù†Ø¯ Ø§Ù†ØªÙ‡Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª (ØªØ±Ø³Ù„ DataFrame Ø¬Ø§Ù‡Ø²)
    data_loaded = pyqtSignal(str, object) # dataset_id, pandas DataFrame
    
    # Ø¥Ø´Ø§Ø±Ø© Ø¹Ù†Ø¯ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„
    load_failed = pyqtSignal(str, str) # dataset_id, error_message
    
    # Ø¥Ø´Ø§Ø±Ø© Ø¹Ù†Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    export_completed = pyqtSignal(str) # file_path

    _instance = None
    _lock = QMutex()

    def __init__(self):
        super().__init__()
        if AlphaDataManager._instance is not None:
            raise Exception("DataManager is a Singleton!")

        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯Ù‚Ø© Ù…Ù† ConfigProvider
        self.data_root = config.project_root / "data"
        self.backtest_dir = self.data_root / "storage" / "backtest"
        self.logs_dir = self.data_root / "logs" / "sessions"
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
        self._ensure_directories()
        
        logger_sink.log_system_event("DataManager", "INFO", "ğŸ“š Archives & Records System Online.")

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = AlphaDataManager()
        return cls._instance

    def _ensure_directories(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹"""
        for path in [self.backtest_dir, self.logs_dir]:
            path.mkdir(parents=True, exist_ok=True)

    # =========================================================================
    # 1. Historical Data Management (Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©)
    # =========================================================================
    def list_available_datasets(self) -> List[Dict[str, Any]]:
        """
        Ù…Ø³Ø­ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©.
        """
        datasets = []
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª Parquet Ùˆ CSV
        extensions = ['*.parquet', '*.csv']
        files = []
        for ext in extensions:
            files.extend(self.backtest_dir.glob(ext))
            
        for f in files:
            stats = f.stat()
            datasets.append({
                "name": f.name,
                "path": str(f),
                "size_mb": round(stats.st_size / (1024 * 1024), 2),
                "modified": datetime.fromtimestamp(stats.st_mtime).strftime("%Y-%m-%d %H:%M"),
                "type": f.suffix.upper().replace('.', '')
            })
        return datasets

    def load_dataset_async(self, file_name: str):
        """
        ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª Ø¶Ø®Ù… ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© (Ù„Ù…Ù†Ø¹ ØªØ¬Ù…ÙŠØ¯ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©).
        """
        file_path = self.backtest_dir / file_name
        if not file_path.exists():
            self.load_failed.emit(file_name, "File not found")
            return

        logger_sink.log_system_event("DataManager", "INFO", f"â³ Loading dataset: {file_name}...")

        # ØªÙÙˆÙŠØ¶ Ø§Ù„Ù…Ù‡Ù…Ø© Ù„Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ù‡Ø§Ù… (Workers)
        task_manager.start_task(
            self._worker_load_data,
            file_path,
            on_result=lambda df: self._on_data_loaded(file_name, df),
            on_error=lambda err: self._on_load_error(file_name, err)
        )

    def _worker_load_data(self, file_path: Path) -> pd.DataFrame:
        """
        Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠØ¹Ù…Ù„ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø®ÙŠØ· Ø§Ù„Ø®Ù„ÙÙŠ (Worker Thread).
        """
        # Forensic Check: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¬Ù… Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„
        file_size = file_path.stat().st_size
        if file_size > 2 * 1024 * 1024 * 1024: # 2 GB Limit
            raise Exception("File too large for direct memory loading. Use chunking.")

        if file_path.suffix == '.parquet':
            df = pd.read_parquet(file_path)
        elif file_path.suffix == '.csv':
            df = pd.read_csv(file_path, parse_dates=True)
        else:
            raise Exception("Unsupported file format")

        # Forensic Check: Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø­ÙŠÙˆÙŠØ©
        required_cols = {'timestamp', 'open', 'high', 'low', 'close', 'volume'}
        # ØªØ­ÙˆÙŠÙ„ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù„Ù„ØµØºÙŠØ±Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        df.columns = [c.lower() for c in df.columns]
        if not required_cols.issubset(df.columns):
            missing = required_cols - set(df.columns)
            raise Exception(f"Corrupted Dataset. Missing critical columns: {missing}")

        return df

    def _on_data_loaded(self, name: str, df: pd.DataFrame):
        """Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„Ø®ÙŠØ· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
        logger_sink.log_system_event("DataManager", "SUCCESS", f"âœ… Dataset loaded: {name} ({len(df)} rows)")
        self.data_loaded.emit(name, df)

    def _on_load_error(self, name: str, error_tuple: tuple):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø®Ø·Ø£"""
        error_msg = str(error_tuple[1])
        logger_sink.log_system_event("DataManager", "ERROR", f"âŒ Failed to load {name}: {error_msg}")
        self.load_failed.emit(name, error_msg)

    # =========================================================================
    # 2. Forensic Archiving (Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠØ©)
    # =========================================================================
    def save_session_snapshot(self):
        """
        Ø¥Ù†Ø´Ø§Ø¡ "ØµÙˆØ±Ø© Ù†Ø¸Ø§Ù…" (System Snapshot) ÙÙˆØ±ÙŠØ©.
        ØªØ­ÙØ¸: Ø§Ù„Ø³Ø¬Ù„Ø§ØªØŒ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø£Ø³ÙˆØ¯ Ù…Ù† StreamHandlerØŒ ÙˆØ­Ø§Ù„Ø© StateStore.
        ØªØ³ØªØ®Ø¯Ù… Ø¹Ù†Ø¯ Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ø£Ùˆ Ø¹Ù†Ø¯ Ø­Ø¯ÙˆØ« Ø®Ø·Ø£ ÙƒØ§Ø±Ø«ÙŠ.
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        snapshot_dir = self.logs_dir / f"session_{timestamp}"
        snapshot_dir.mkdir(parents=True, exist_ok=True)

        logger_sink.log_system_event("DataManager", "WARNING", f"ğŸ’¾ Creating Forensic Snapshot in: {snapshot_dir}")

        task_manager.start_task(
            self._worker_save_snapshot,
            snapshot_dir,
            on_finished=lambda: self.export_completed.emit(str(snapshot_dir))
        )

    def _worker_save_snapshot(self, folder: Path):
        """ØªÙ†ÙÙŠØ° Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
        
        # 1. ØªÙØ±ÙŠØº Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø£Ø³ÙˆØ¯ (Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ø§Ù„Ø£Ø®ÙŠØ±Ø©)
        raw_data = stream_handler.dump_blackbox()
        raw_df = pd.DataFrame(raw_data, columns=['timestamp', 'ticker', 'price', 'volume'])
        raw_df.to_csv(folder / "blackbox_stream.csv", index=False)

        # 2. Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ (Audit Trail)
        # Ù†Ø­ØªØ§Ø¬ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù€ StateStore Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø³Ø¬Ù„ (ÙŠØ¬Ø¨ Ø§Ø³ØªÙŠØ±Ø§Ø¯Ù‡ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„ØªØ¬Ù†Ø¨ Circular Import Ø¥Ø°Ø§ Ø£Ù…ÙƒÙ†ØŒ 
        # Ø£Ùˆ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¹Ù„ÙˆÙŠ)
        from ui.core.state_store import state_store 
        history = state_store.get_history()
        with open(folder / "state_audit.json", "w", encoding="utf-8") as f:
            json.dump(history, f, indent=4, ensure_ascii=False)

        # 3. Ø­ÙØ¸ Ù…Ù„Ù checksum Ù„Ù„Ø£Ù…Ø§Ù†
        self._generate_checksum(folder / "blackbox_stream.csv")

    def _generate_checksum(self, file_path: Path):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ØµÙ…Ø© Ø±Ù‚Ù…ÙŠØ© Ù„Ù„Ù…Ù„Ù Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø§Ù„ØªÙ„Ø§Ø¹Ø¨ Ø¨Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        
        checksum_file = file_path.with_suffix('.sha256')
        with open(checksum_file, "w") as f:
            f.write(sha256_hash.hexdigest())

# Global Accessor
data_manager = AlphaDataManager.get_instance()